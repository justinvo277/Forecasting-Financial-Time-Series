
  0%|                                                                                          | 0/897 [00:00<?, ?it/s]
  0%|                                                                                          | 0/897 [00:00<?, ?it/s]D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)





























 99%|███████████████████████████████████████████████████████████████████▍| 890/897 [01:02<00:00, 14.67it/s, MSE=0.0116]
[1/300: TRAINING PHASE]: MSE: 0.021824082560835192
100%|███████████████████████████████████████████████████████████████████| 897/897 [01:02<00:00, 14.29it/s, MSE=0.00749]

 72%|█████████████████████████████████████████████████████████▏                      | 123/172 [00:03<00:01, 40.94it/s]
[1/300: VALIDATION PHASE]: MSE: 0.008807733927062864 | MAE: 0.07503756226668525 | HUBER: 0.004403866963531432
Validation loss decreased (inf --> 0.008808).  Saving model ...
100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [00:04<00:00, 40.24it/s]































 98%|█████████████████████████████████████████████████████████████████▉ | 883/897 [01:03<00:00, 16.44it/s, MSE=0.00241]
[2/300: TRAINING PHASE]: MSE: 0.005053207673181218
100%|████████████████████████████████████████████████████████████████████| 897/897 [01:04<00:00, 14.01it/s, MSE=0.0037]






























 99%|██████████████████████████████████████████████████████████████████ | 885/897 [01:01<00:00, 16.99it/s, MSE=0.00126]
[3/300: TRAINING PHASE]: MSE: 0.0023245292839606125
100%|███████████████████████████████████████████████████████████████████| 897/897 [01:02<00:00, 14.40it/s, MSE=0.00189]












 46%|██████████████████████████████▊                                    | 413/897 [00:26<00:31, 15.49it/s, MSE=0.00196]
Traceback (most recent call last):
  File "D:\-DSP391m-Forecasting-Financial-Time-Series-With-Transformer\train.py", line 110, in <module>
    loss = train_loop(model=model, datatrain=train_data, opt=opt, criterion=mse_loss,
  File "D:\-DSP391m-Forecasting-Financial-Time-Series-With-Transformer\utils.py", line 28, in train_loop
    output = model(src=src, tgt=trg, src_mask=src_mask, tgt_mask=tgt_mask)
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\-DSP391m-Forecasting-Financial-Time-Series-With-Transformer\transformer_model.py", line 178, in forward
    decoder_output = self.decoder(
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\transformer.py", line 494, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\transformer.py", line 891, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\transformer.py", line 909, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\modules\activation.py", line 1266, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "D:\Anaconda\envs\dsp391m\lib\site-packages\torch\nn\functional.py", line 5496, in multi_head_attention_forward
    attn_mask = attn_mask.unsqueeze(0)
KeyboardInterrupt