
  6%|██████████▏                                                                                                                                                      | 25/393 [00:01<00:19, 18.61it/s, MSE=0.0611]









 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 359/393 [00:20<00:01, 18.60it/s, MSE=0.0244]
[RESULT TRAINING PHASE]: MSE: 0.04065335084833489
[1/300: VALIDATION PHASE]

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 393/393 [00:21<00:00, 17.92it/s, MSE=0.0266]
  0%|                                                                                                                                                                                      | 0/172 [00:00<?, ?it/s]c:\Users\Cong\.conda\envs\idm\lib\site-packages\torch\nn\modules\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\attention.cpp:152.)
  return torch._native_multi_head_attention(
